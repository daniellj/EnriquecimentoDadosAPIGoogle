{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fechando o arquivo de origem /home/daniellj/Projetos/ETL_Python/data/data_points.txt\n",
      "Fechando e excluindo o arquivo temporário /home/daniellj/Projetos/ETL_Python/data/data_points_20180331_080420.tmp\n",
      "JSONDecodeError: decoding JSON has failed (Latitude: -30.02818053 | Longituide: -51.18667311 )\n",
      "JSONDecodeError: decoding JSON has failed (Latitude: -30.01523721 | Longituide: -51.17616808 )\n",
      "Processo de carga finalizado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#from conexao_bd import string_bd\n",
    "from io import open\n",
    "from os import listdir, system, remove\n",
    "from os.path import isfile, join\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import threading\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "filepath = \"/home/daniellj/Projetos/ETL_Python/data/\"\n",
    "filename = \"data_points\"\n",
    "extension = \"txt\"\n",
    "APIKEYGOOGLEMAPS = 'AIzaSyAgaYMWeeU4aM1MaTVb40uuNBtbZNo8PLQ' #your api key (Google)\n",
    "TABLENAME = 'geolocalizacao'\n",
    "DB_HOST = 'Ubuntu-WxInvestment'\n",
    "DB_PORT = 3306\n",
    "DB_USER = 'geolocalizacao'\n",
    "DB_PASS = 'Geolx!177'\n",
    "DB_NAME = 'geolocalizacao'\n",
    "compare=['latitude', 'longitude']\n",
    "\n",
    "def files_to_process(path, name, ext):\n",
    "    '''\n",
    "    Procura em uma determinada pasta do S.O. por arquivos com um nome + extensão padrão.\n",
    "    Retorna uma lista com os arquivos encontrados.\n",
    "    '''\n",
    "    files_find = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if (os.path.isfile(os.path.join(path, file))) and (file == name + '.' + ext):\n",
    "            files_find.append(files)\n",
    "    return (files_find)\n",
    "\n",
    "def complete_getdate():\n",
    "    '''\n",
    "    Função que retorna o getdate() em formato string, com a máscara YYYYMMDD_HHMMSS.\n",
    "    '''\n",
    "    time_str = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return (str(time_str))\n",
    "\n",
    "def create_coordenate_lists(filepath_source, filename_source, ext_source):\n",
    "    '''\n",
    "    Input: necessita de valores de entrada:\n",
    "        - filepath_source = pasta onde se localiza o arquivo de entrada.\n",
    "        - filename_source = nome do arquivo à ser processado.\n",
    "        - ext_source = extensão do arquivo.\n",
    "    Output: Retorna uma tupla com duas listas: latitude e longitude.\n",
    "    '''\n",
    "    file_source = filepath_source + filename_source + '.' + ext_source\n",
    "    file_temp = filepath_source + filename_source + '_' + complete_getdate() + '.tmp'\n",
    "    # print(file_source)\n",
    "    # print(file_temp)\n",
    "\n",
    "    try:\n",
    "        # abrindo os arquivos de origem e temporário\n",
    "        file_src = open(file_source, 'r')\n",
    "        file_tmp = open(file_temp, 'w+')\n",
    "\n",
    "        # escrevendo o conteúdo do arquivo de origem no temporário\n",
    "        for line_src in file_src.readlines():\n",
    "            file_tmp.write(line_src)\n",
    "\n",
    "        # retornando a linha inicial do arquivo de origem\n",
    "        file_tmp.seek(0, 0)\n",
    "\n",
    "        # gerando uma lista de elemtos à partir de valores do arquivo temporário\n",
    "        lista_inicial = file_tmp.readlines()\n",
    "\n",
    "        # criando uma lista para cada uma das coordenadas: Latitude e Longitude\n",
    "        tamanho = len(lista_inicial)\n",
    "        posicao = 0\n",
    "        lista_latitude = []\n",
    "        lista_longitude = []\n",
    "\n",
    "        for item in lista_inicial:\n",
    "            if posicao <= tamanho:\n",
    "                if 'Latitude' in item:\n",
    "                    latitude = (item[23:len(item)])\n",
    "                    lista_latitude.append(latitude.replace('\\n', ''))\n",
    "                if 'Longitude' in item:\n",
    "                    longitude = (item[24:len(item)])\n",
    "                    lista_longitude.append(longitude.replace('\\n', ''))\n",
    "            posicao = posicao + 1\n",
    "\n",
    "    finally:\n",
    "        print('Fechando o arquivo de origem', file_source)\n",
    "        file_src.closed\n",
    "        print('Fechando e excluindo o arquivo temporário', file_temp)\n",
    "        file_tmp.closed\n",
    "        remove(file_temp)\n",
    "    return (lista_latitude, lista_longitude)\n",
    "\n",
    "def recursive_geocode_googlemaps(latitude, longitude):\n",
    "    '''\n",
    "    Função que recebe como parâmetro a localização geográfica no formato de geolocalização (latitude e longitude),\n",
    "    e retorna as informações básicas de um endereço em um DataFrame Pandas:\n",
    "    - latitude\n",
    "    - longitude\n",
    "    - rua\n",
    "    - numero\n",
    "    - bairro\n",
    "    - cidade\n",
    "    - cep\n",
    "    - estado\n",
    "    - pais\n",
    "    - enderecocompleto\n",
    "    '''\n",
    "    sensor = 'true'\n",
    "    result_type = 'street_address|street_number|country|administrative_area_level_1|administrative_area_level_2|postal_code' # 'street_address|street_number|country|administrative_area_level_1|administrative_area_level_2|postal_code'\n",
    "    language = 'pt' #en = inglês\n",
    "    location_type='ROOFTOP'\n",
    "    apikey = APIKEYGOOGLEMAPS\n",
    "\n",
    "    base = \"https://maps.googleapis.com/maps/api/geocode/json?\"\n",
    "    params = \"latlng={lat},{lon}&sensor={sen}&result_type={result_type}&language={language}&location_type={location_type}\".format(\n",
    "        lat=latitude,\n",
    "        lon=longitude,\n",
    "        sen=sensor,\n",
    "        result_type=result_type,\n",
    "        language=language,\n",
    "        location_type=location_type\n",
    "    )\n",
    "    key = \"&key={apikey}\".format(apikey=apikey)\n",
    "    url = \"{base}{params}{key}\".format(base=base, params=params, key=key)\n",
    "    \n",
    "    #entregando a resposta da consulta para um objeto response\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    try:\n",
    "        #transformando o conteúdo JSON para um objeto STRING(DICT) em Python\n",
    "        data_str = str(response.json()).replace(\"'\",'\"')\n",
    "    \n",
    "        resp_dict = json.loads(data_str)\n",
    "\n",
    "        #Buscando o resultado da consulta à API do Google Maps\n",
    "        status = resp_dict['status']\n",
    "\n",
    "        #Se o retorno da consulta for \"OK\", prossegue com o tratamento, senão encerra\n",
    "        #if (\"ZERO_RESULTS\" in status or \"OVER_QUERY_LIMIT\" in status or \"REQUEST_DENIED\" in status or \"INVALID_REQUEST\" in status or \"UNKNOWN_ERROR\" in status):\n",
    "            #print('Coordenada com problema. Latitude:', '|', latitude, ' Longitude:', longitude, '|', ' Status:', status)\n",
    "        if (\"OK\" in status):\n",
    "            #processo para inserir os valores coletados em um DataFrame Pandas...\n",
    "            list_values = []\n",
    "            cep_default = ''\n",
    "            list_values.append(latitude) #latitude\n",
    "            list_values.append(longitude) #longitude\n",
    "\n",
    "            for item in resp_dict['results'][0]['address_components']:\n",
    "                if 'route' in item['types']: #rua\n",
    "                    list_values.append(item['long_name'])\n",
    "                if 'street_number' in item['types']: #numero\n",
    "                    list_values.append(item['long_name'])\n",
    "                if 'sublocality_level_1' in item['types']: #bairro\n",
    "                    list_values.append(item['long_name'])\n",
    "                if 'administrative_area_level_2' in item['types']: #cidade\n",
    "                    list_values.append(item['long_name'])\n",
    "            if 'postal_code' not in item['types']: #cep\n",
    "                list_values.append(cep_default)\n",
    "            else:\n",
    "                list_values.append(item['long_name'])\n",
    "            for item in resp_dict['results'][0]['address_components']:\n",
    "                if 'administrative_area_level_1' in item['types']: #estado\n",
    "                    list_values.append(item['long_name'])\n",
    "                if 'country' in item['types']: #pais\n",
    "                    list_values.append(item['long_name'])\n",
    "            list_values.append(resp_dict['results'][0]['formatted_address'])\n",
    "            return(list_values)\n",
    "        #address_tup = {(key, value) for (key, value) in zip(list_key, list_values)}    \n",
    "        #transformando a Tupla em um Dicionário, para que possa ser importado no Pandas...\n",
    "        #address_dict = dict(address_tup)\n",
    "        #address_df = pd.DataFrame.from_dict([address_dict])\n",
    "\n",
    "    except json.JSONDecodeError: # JSONDecodeError\n",
    "        print('JSONDecodeError: decoding JSON has failed (Latitude:', latitude,'|', 'Longituide:', longitude,')')\n",
    "\n",
    "def dataframe_input():\n",
    "    '''\n",
    "    Preparando o dataframe com base nas informações de endereços processadas pela chamada da função create_coordenate_lists().\n",
    "    '''\n",
    "    #chamando a função create_coordenate_lists() para ter as duas listas populadas: latitude e longitude.\n",
    "    tup = create_coordenate_lists(filepath_source = filepath, filename_source = filename, ext_source = extension)\n",
    "    \n",
    "    address_values = []\n",
    "    latitude = []\n",
    "    longitude = []\n",
    "    \n",
    "    columns = ['latitude', 'longitude', 'numero', 'rua', 'bairro', 'cidade', 'estado', 'pais', 'cep', 'enderecocompleto']\n",
    "\n",
    "    tamanho = len(tup[0])\n",
    "    \n",
    "    for item in tup[0]:\n",
    "        latitude.append(item)\n",
    "    #print(latitude)\n",
    "    \n",
    "    for item in tup[1]:\n",
    "        longitude.append(item)\n",
    "    #print(latitude)\n",
    "    \n",
    "    df_latitude = pd.DataFrame(latitude)\n",
    "    df_longitude = pd.DataFrame(longitude)\n",
    "    frames = [df_latitude, df_longitude]\n",
    "\n",
    "    df = pd.concat(frames, axis=1)\n",
    "    list_values = df.values.tolist()\n",
    "\n",
    "    for item in list_values:\n",
    "        latitude = float(item[0])\n",
    "        longitude = float(item[1])\n",
    "        address_values.append(recursive_geocode_googlemaps(latitude=latitude, longitude=longitude))\n",
    "\n",
    "    #removendo valores \"None\"\n",
    "    address_values = [x for x in address_values if x is not None]\n",
    "    \n",
    "    #concatenando os dados com as colunas, para formar o DataFrame\n",
    "    df_address = (pd.DataFrame(data=address_values, columns=columns)).sort_values(by=compare, ascending=True)\n",
    "    return(df_address)\n",
    "\n",
    "def string_connection_bd(host, port, database, user, password):\n",
    "    ''' SqlAlchemy Config. '''\n",
    "    DB_TYPE = 'mysql'\n",
    "    DB_DRIVER = 'pymysql'\n",
    "\n",
    "    ''' Default Config. '''\n",
    "    DB_HOST = host\n",
    "    DB_PORT = port\n",
    "    DB_NAME = database\n",
    "    DB_USER = user\n",
    "    DB_PASS = password\n",
    "    #CHARSET = 'latin1'\n",
    "    #CURSORCLASS = 'pymysql.cursors.Cursor'\n",
    "\n",
    "    ''' SqlAlchemy Config. '''\n",
    "    connection = '%s+%s://%s:%s@%s:%s/%s' % (DB_TYPE, DB_DRIVER, DB_USER, DB_PASS, DB_HOST, DB_PORT, DB_NAME)\n",
    "    return (connection)\n",
    "\n",
    "def dataframe_output(df, engine, tablename, dup_cols=[], filter_continuous_col=None, filter_categorical_col=None):\n",
    "    \"\"\"\n",
    "    Remove linhas de um dataframe que já exista dentro de uma tabela do banco de dados.\n",
    "    \n",
    "    Requerimentos:\n",
    "        df : dataframe inicial que se deseja remover as linhas já existentes em relação ao conteúdo de uma tabela do BD.\n",
    "        engine: SQLAlchemy engine object (ver documentação).\n",
    "        tablename: nome da tabela base para checar o conteúdo já existente em relação ao dataframe inicial.\n",
    "        dup_cols: lista ou tupla de nomes de colunas do dataframe inicial onde os valores serão comparados com os da tabela\n",
    "                  do BD para remoção dos dados já existentes no dataframe.\n",
    "    \n",
    "    Opcional:\n",
    "        filter_continuous_col: nome das colunas da tabela do BD para dados que sejam continuos, para uso de filtros do\n",
    "                               tipo BETWEEEN min/max. Aceita dados das seguintes tipagens: : datetime, int, or float.\n",
    "        filter_categorical_col : nome de colunas categóricas da tabela do BD, para uso de filtros do tipo WHERE =\n",
    "                                \"valor esperado\". Aceita valores distintos dentro da avaliação da expressão IN ('valor'),\n",
    "                                em relação à deteminada coluna de uma tabela do banco de dados.\n",
    "    Retorno:\n",
    "        Uma lista única de valores do dataframe onde os valores comparados com os da tabela do BD,\n",
    "        não tem tem um correspondente. Resumindo, uma lista de valores que ainda não existem na tabela comparada do BD.\n",
    "    \"\"\"\n",
    "    args = 'SELECT %s FROM %s.%s' %(', '.join(['{0}'.format(col) for col in dup_cols]), DB_NAME, tablename)\n",
    "    args_contin_filter, args_cat_filter = None, None\n",
    "    if filter_continuous_col is not None:\n",
    "        if df[filter_continuous_col].dtype == 'datetime64[ns]':\n",
    "            args_contin_filter = \"\"\" \"%s\" BETWEEN Convert(datetime, '%s') \n",
    "                                          AND Convert(datetime, '%s')\"\"\" %(filter_continuous_col, \n",
    "                              df[filter_continuous_col].min(), df[filter_continuous_col].max())\n",
    "\n",
    "    if filter_categorical_col is not None:\n",
    "        args_cat_filter = ' \"%s\" in(%s)' %(filter_categorical_col, \n",
    "                          ', '.join([\"'{0}'\".format(value) for value in df[filter_categorical_col].unique()]))\n",
    "\n",
    "    if args_contin_filter and args_cat_filter:\n",
    "        args += ' Where ' + args_contin_filter + ' AND' + args_cat_filter\n",
    "    elif args_contin_filter:\n",
    "        args += ' Where ' + args_contin_filter\n",
    "    elif args_cat_filter:\n",
    "        args += ' Where ' + args_cat_filter\n",
    "\n",
    "    df.drop_duplicates(subset=dup_cols, keep='last', inplace=True) #inplace = descarta duplicadas no local\n",
    "    \n",
    "    #busca no banco de dados os registros para comparar com os dados de input...\n",
    "    df_query_result = (pd.read_sql(con=engine, sql=args, columns=dup_cols)).sort_values(by=dup_cols, ascending=True)\n",
    "\n",
    "    df = pd.merge(df, df_query_result, how='left', on=dup_cols, indicator=True)\n",
    "    df = df[df['_merge'] == 'left_only']\n",
    "    df.drop(['_merge'], axis=1, inplace=True)\n",
    "    return(df)\n",
    "\n",
    "def insert_data_db(df, pool_size, *args, **kargs):\n",
    "    \"\"\"\n",
    "    Extensão do método to_sql() da biblioteca PANDAS do Python, para inserções em tabelas de um BD usando thread.\n",
    "\n",
    "    Requerimentos: \n",
    "        df : dataframe em PANDAS que se deseja inserir linhas em uma tabela de um banco de dados.\n",
    "        POOL_SIZE : Configuração do parâmetro \"max connection pool size\" do sqlalchemy .\n",
    "            Sugestão: setar valor < que a conexão do banco de dados.\n",
    "    *args:\n",
    "        Argumentos do método to_sql() para PANDAS.\n",
    "        Argumentos requeridos de to_SQL():\n",
    "            con : SqlAlchemy engine\n",
    "            name : Nome da tabela do banco de dados que se deseja inserir as linhas carregadas pelo dataframe.\n",
    "\n",
    "        Argumentos opcionais:\n",
    "            'if_exists' : 'append' ou 'replace'.  Se a tabela já existe, usar a opção 'append'.\n",
    "            'index' : True ou False. True se você quer escrever os valores dos índices para o banco de dados.\n",
    "\n",
    "    Créditos para o código usando threading em Python, ao qual fiz adaptações:\n",
    "        http://techyoubaji.blogspot.com/2015/10/speed-up-pandas-tosql-with.html\n",
    "    \"\"\"\n",
    "\n",
    "    CHUNKSIZE = 500\n",
    "    INITIAL_CHUNK = 100\n",
    "    if len(df) > CHUNKSIZE:\n",
    "        '''\n",
    "        insere no banco de dados a quantidade de registros iniciais propostas pela variável INITIAL_CHUNK, somente se df for maior que\n",
    "        o valor proposto pela variável CHUNKSIZE.\n",
    "        '''\n",
    "        df.iloc[:INITIAL_CHUNK, :].to_sql(*args, **kargs)\n",
    "    else:\n",
    "        '''\n",
    "        caso a quantidade de registros do df for menor que o valor proposto pela variável CHUNKSIZE, então simplesmente insere no banco\n",
    "        de dados os registros.\n",
    "        '''\n",
    "        df.to_sql(*args, **kargs)\n",
    "\n",
    "    workers, i, j = [], 0, 1\n",
    "\n",
    "    '''\n",
    "    o iterador abaixo verifica quantos blocos de registros (valor de CHUNKSIZE) ele irá inserir por vez usando thread para melhora\n",
    "    de performance nas operações de insert com grande volume de linhas.\n",
    "    '''\n",
    "    if ((df.shape[0] - INITIAL_CHUNK)//CHUNKSIZE)>0:\n",
    "        for i in range((df.shape[0] - INITIAL_CHUNK)//CHUNKSIZE):\n",
    "            t = threading.Thread(target=lambda: df.iloc[INITIAL_CHUNK+i*CHUNKSIZE:INITIAL_CHUNK+(i+1)*CHUNKSIZE].to_sql(*args, **kargs))\n",
    "            t.start()\n",
    "            workers.append(t)\n",
    "            df.iloc[INITIAL_CHUNK+(i+1)*CHUNKSIZE:, :].to_sql(*args, **kargs)\n",
    "            [t.join() for t in workers]\n",
    "    elif (df.shape[0] - INITIAL_CHUNK)>0:\n",
    "        '''\n",
    "        caso a quantidade de blocos de registros (valor de CHUNKSIZE) seja insignificante (menor do que 1), então insere os registros no\n",
    "        banco de dados a partir da posição INITIAL_CHUNK+i, onde j=1. Ou seja, insere o restante dos registros.\n",
    "        '''\n",
    "        df.iloc[INITIAL_CHUNK+j:].to_sql(*args, **kargs)\n",
    "\n",
    "def main(filepath, filename, extension, DB_HOST, DB_PORT, DB_NAME, TABLENAME, DB_USER, DB_PASS):\n",
    "    \"\"\"\n",
    "    Função que faz a chamada do processo de ETL proposto.\n",
    "    \"\"\"\n",
    "\n",
    "    ###SqlAlchemy Config.###\n",
    "    POOL_SIZE = 1000\n",
    "\n",
    "    SQLALCHEMY_DATABASE_URI = string_connection_bd(host = DB_HOST, port = DB_PORT, database = DB_NAME, user = DB_USER, password = DB_PASS)\n",
    "    ENGINE = create_engine(SQLALCHEMY_DATABASE_URI, pool_size=POOL_SIZE, max_overflow=0)\n",
    "\n",
    "    #chamando a função recursive_geocode_googlemaps(), onde para cada coordenada geográfica contida no objeto\n",
    "    #tup (do tiplo TUPLA), é buscado o endereço completo junto a API do Google Maps e, junto a função insert_data_db,\n",
    "    #é inserido no banco de dados os registros.\n",
    "\n",
    "    #insert_data_db(df = dataframe_input(), pool_size=POOL_SIZE, con=ENGINE, name=TABLENAME, if_exists='append', index=False)\n",
    "    \n",
    "    #inserindo dados comparando se já existe no banco!\n",
    "    df_input = dataframe_input()\n",
    "    df_output = dataframe_output( df = df_input, engine=ENGINE, tablename=TABLENAME, dup_cols=compare)\n",
    "    #print(df_output)\n",
    "\n",
    "    insert_data_db( df = df_output, pool_size=POOL_SIZE, con=ENGINE, name=TABLENAME, if_exists='append', index=False)\n",
    "    print('Processo de carga finalizado com sucesso!')\n",
    "\n",
    "main(filepath = filepath, filename = filename, extension = extension, DB_HOST = DB_HOST, DB_PORT = DB_PORT, DB_NAME = DB_NAME, TABLENAME = TABLENAME, DB_USER = DB_USER, DB_PASS = DB_PASS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
